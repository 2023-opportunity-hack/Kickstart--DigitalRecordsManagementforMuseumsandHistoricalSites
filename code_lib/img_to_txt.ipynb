{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IRMGPPFOCLT",
        "outputId": "90878766-96c7-4ac2-a19f-56500af246ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are two women standing outside of a gazebo in the yard\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "\n",
        "img_url = 'Carlson.Anderson.acq0001.jpg'\n",
        "raw_image = Image.open(img_url).convert('RGB')\n",
        "\n",
        "# conditional image captioning\n",
        "# text = \"a photography of\"\n",
        "# inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
        "\n",
        "# out = model.generate(**inputs)\n",
        "# print(processor.decode(out[0], skip_special_tokens=True))\n",
        "\n",
        "# unconditional image captioning\n",
        "inputs = processor(raw_image, return_tensors=\"pt\")\n",
        "\n",
        "out = model.generate(**inputs)\n",
        "print(processor.decode(out[0], skip_special_tokens=True))\n"
      ]
    }
  ]
}